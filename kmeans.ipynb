{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handy-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "import numpy as np\n",
    "from tweepy import OAuthHandler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8de553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ./auth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "departmental-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClient(object):\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.auth = OAuthHandler(os.getenv('api_key'), os.getenv('api_secret'))\n",
    "            self.auth.set_access_token(os.getenv('oauth_token'), os.getenv('oauth_token_secret'))\n",
    "            self.api = tweepy.API(self.auth)\n",
    "        except Tweepy.TweepError:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "\n",
    "    def get_tweets(self, query, count = 10):\n",
    "        tweets = []\n",
    "        try:\n",
    "            fetched_tweets = self.api.search_tweets(q = query, count = count)\n",
    "            for tweet in fetched_tweets:\n",
    "                if tweet.text not in tweets:\n",
    "                    tweets.append(tweet.text)\n",
    "            return tweets\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Error : \" + str(e))\n",
    "            \n",
    "    def get_trending_tags(self):\n",
    "        trends1 = self.api.get_place_trends(1)\n",
    "        data = trends1[0] \n",
    "        trends = data['trends']\n",
    "        names = []\n",
    "        for trend in trends:\n",
    "            if trend['name'][0]=='#':\n",
    "                names.append(trend['name'])\n",
    "        return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attended-seventh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#AEWDynamite', '#Survivor', '#LakeShow', '#FestaAFazenda', '#VijayDiwas']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = TwitterClient()\n",
    "# api.get_tweets(query = '#chess', count = 200)\n",
    "api.get_trending_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stock-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as f:\n",
    "    tweets = api.get_tweets('#FarmLaws', 20000)\n",
    "    for tweet in tweets:\n",
    "        f.write(tweet + \"_$_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e51c00",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f59b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt', 'r')\n",
    "tweets = f.read().split('_$_')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7da8d",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbcb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import download\n",
    "# download('stopwords')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    tweets_eng = []\n",
    "    for word in tweets_clean:\n",
    "        flag = True\n",
    "        for i in word:\n",
    "            if ord(i) >= 256:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            tweets_eng.append(word)\n",
    "    return tweets_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05fd7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = [process_tweet(tweet) for tweet in tweets]\n",
    "while [] in processed_tweets:\n",
    "    processed_tweets.remove([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "637797be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bceb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tweet:list, freqs:dict, wordToTweet:dict):\n",
    "    for word in tweet:\n",
    "        if word in freqs:\n",
    "            freqs[word] += 1\n",
    "            wordToTweet[word].append(tweet)\n",
    "        else:\n",
    "            freqs[word] = 1\n",
    "            wordToTweet[word] = [tweet]\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f75071",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = dict()\n",
    "wordToTweet = dict()\n",
    "for tweet in processed_tweets:\n",
    "    count_words(tweet, freqs, wordToTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e23828b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'touch': 2,\n",
       " 'moment': 2,\n",
       " '...': 2,\n",
       " 'daughter': 2,\n",
       " 'hug': 2,\n",
       " 'father': 2,\n",
       " 'return': 4,\n",
       " 'delhi': 3,\n",
       " 'win': 2,\n",
       " 'long': 4,\n",
       " 'battl': 2,\n",
       " 'farmer': 11,\n",
       " 'victori': 2,\n",
       " 'farmersprotest_fatehma': 1,\n",
       " 'farmlaw': 22,\n",
       " 'backbon': 1,\n",
       " 'india': 5,\n",
       " \"can't\": 1,\n",
       " 'decid': 1,\n",
       " 'crop': 1,\n",
       " 'cost': 1,\n",
       " 'incredibleindia': 1,\n",
       " 'ye': 1,\n",
       " 'anoth': 1,\n",
       " 'bill': 1,\n",
       " 'like': 2,\n",
       " 'indian': 1,\n",
       " 'gather': 1,\n",
       " 'voic': 1,\n",
       " 'reveal': 1,\n",
       " 'true': 1,\n",
       " 'valu': 1,\n",
       " 'produc': 1,\n",
       " 'adjust': 1,\n",
       " 'ineffici': 1,\n",
       " 'leakag': 1,\n",
       " 'loss': 1,\n",
       " 'given': 1,\n",
       " 'decent': 1,\n",
       " 'burial': 1,\n",
       " 'deserv': 1,\n",
       " 'govern': 1,\n",
       " 'bid': 1,\n",
       " 'food-gr': 1,\n",
       " 'farm': 2,\n",
       " 'modi': 1,\n",
       " 'exclus': 2,\n",
       " 'msp': 3,\n",
       " 'http': 1,\n",
       " ':/': 1,\n",
       " '5': 1,\n",
       " '10': 1,\n",
       " 'live': 1,\n",
       " 'baattochubhegi': 1,\n",
       " 'justiceforlakhimpurfarm': 2,\n",
       " 'mera': 1,\n",
       " 'bhai': 1,\n",
       " 'maar': 1,\n",
       " 'diya': 1,\n",
       " 'gund': 1,\n",
       " 'ne': 1,\n",
       " 'chief': 1,\n",
       " 'economist': 1,\n",
       " 'deputi': 1,\n",
       " 'md': 1,\n",
       " 'imf': 1,\n",
       " 'geetagopinath': 1,\n",
       " 'support': 1,\n",
       " 'anti': 1,\n",
       " 'peopl': 4,\n",
       " 'tikait': 2,\n",
       " 'wan': 1,\n",
       " 'haryana': 1,\n",
       " 'undertook': 1,\n",
       " '-10,000': 1,\n",
       " 'tractor': 1,\n",
       " 'march': 1,\n",
       " 'call': 1,\n",
       " 'bku': 1,\n",
       " 'leader': 3,\n",
       " 'gurnam': 1,\n",
       " 'singh': 1,\n",
       " 'chaduni': 1,\n",
       " 'th': 3,\n",
       " 'congratul': 1,\n",
       " 'pm': 1,\n",
       " 'ji': 1,\n",
       " 'honor': 1,\n",
       " 'wish': 2,\n",
       " 'democraci': 1,\n",
       " 'prevail': 1,\n",
       " 'obstruct': 1,\n",
       " 'ordinari': 1,\n",
       " 'part': 1,\n",
       " 'parliamentari': 1,\n",
       " 'procedur': 1,\n",
       " 'ivor': 1,\n",
       " 'jen': 1,\n",
       " 'rajyasabha': 1,\n",
       " 'break': 1,\n",
       " 'news': 2,\n",
       " 'mpcg': 1,\n",
       " 'itv': 1,\n",
       " 'indianewsmpcg': 1,\n",
       " 'suspend_dsp_gurmeetsingh': 1,\n",
       " 'farmlawsrep': 2,\n",
       " 'farmersprotest': 2,\n",
       " 'sit': 2,\n",
       " 'quietli': 2,\n",
       " 'ajaymishra': 2,\n",
       " 'teni': 2,\n",
       " 'arrest': 2,\n",
       " 'go': 2,\n",
       " 'misus': 1,\n",
       " 'dr': 1,\n",
       " 'farmerprotest': 1,\n",
       " 'agai': 1,\n",
       " 'press': 1,\n",
       " 'note': 1,\n",
       " 'date': 1,\n",
       " '14th': 1,\n",
       " 'dec': 1,\n",
       " '2021': 1,\n",
       " 'farmerswon': 1,\n",
       " '1yearoffarmersprotest': 1,\n",
       " 'despit': 1,\n",
       " 'prolong': 1,\n",
       " 'struggl': 1,\n",
       " 'controversi': 1,\n",
       " 'misusi': 1,\n",
       " 'repealedfarmlaw': 1,\n",
       " 'farmersvictori': 1,\n",
       " 'madurai': 1,\n",
       " 'repeal': 2,\n",
       " '700': 1,\n",
       " 'death': 1,\n",
       " 'one': 1,\n",
       " 'year': 1,\n",
       " 'protest': 1,\n",
       " 'want': 1,\n",
       " 'freed': 1,\n",
       " 'ghar': 1,\n",
       " 'barbad': 1,\n",
       " 'hone': 1,\n",
       " 'se': 3,\n",
       " 'bachan': 1,\n",
       " 'hain': 2,\n",
       " 'sharab': 1,\n",
       " 'ban': 1,\n",
       " 'honi': 1,\n",
       " 'hi': 1,\n",
       " 'chahiy': 1,\n",
       " 'iss': 1,\n",
       " 'gunda-gardi': 1,\n",
       " 'bhi': 1,\n",
       " 'kafi': 1,\n",
       " 'kam': 1,\n",
       " 'hogi': 1,\n",
       " 'share': 1,\n",
       " 'karo': 1,\n",
       " 'f': 1,\n",
       " 'sarkar': 1,\n",
       " 'ko': 1,\n",
       " 'vyaktivadi': 1,\n",
       " 'na': 1,\n",
       " 'hokar': 1,\n",
       " 'sahyogatmak': 1,\n",
       " 'banakar': 1,\n",
       " 'sabka': 2,\n",
       " 'sath': 1,\n",
       " 'vika': 1,\n",
       " 'ki': 4,\n",
       " 'vhavna': 1,\n",
       " 'sabhi': 1,\n",
       " 'ke': 1,\n",
       " 'sujhavon': 1,\n",
       " 'nirnay': 1,\n",
       " 'l': 1,\n",
       " 'ham': 1,\n",
       " 'laut': 1,\n",
       " 'rahe': 1,\n",
       " 'aap': 2,\n",
       " 'himat': 1,\n",
       " 'takat': 1,\n",
       " 'aapki': 1,\n",
       " 'sahanshilta': 1,\n",
       " 'aapka': 1,\n",
       " 'bharosa': 1,\n",
       " 'sab': 1,\n",
       " 'kuchh': 1,\n",
       " 'himalaya': 1,\n",
       " 'tarah': 1,\n",
       " 'ha': 1,\n",
       " 'rakesh': 1,\n",
       " 'convoy': 1,\n",
       " 'left': 1,\n",
       " 'ghazipur': 1,\n",
       " 'border': 1,\n",
       " 'crowd': 1,\n",
       " 'vehicl': 1,\n",
       " 'also': 1,\n",
       " 'came': 1,\n",
       " 'rakeshtikait': 1,\n",
       " 'welcom': 1,\n",
       " 'meerut': 1,\n",
       " 'upon': 1,\n",
       " 'earlier': 1,\n",
       " 'today': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a29dea07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('farmlaw', 22),\n",
       " ('farmer', 11),\n",
       " ('india', 5),\n",
       " ('return', 4),\n",
       " ('long', 4),\n",
       " ('peopl', 4),\n",
       " ('ki', 4),\n",
       " ('delhi', 3),\n",
       " ('msp', 3),\n",
       " ('leader', 3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_sorted = list(freqs.items())\n",
    "freq_sorted.sort(key = lambda x : -x[1])\n",
    "freq_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d33e098",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048b699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweet, alphabet):\n",
    "    v = np.zeros(len(alphabet))\n",
    "    for i in range(len(alphabet)):\n",
    "        if alphabet[i] in tweet:\n",
    "            v[i] += 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c5a657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closestCluster(vector, centroids):\n",
    "    closest = -1\n",
    "    minDist = 2**30\n",
    "    for key in centroids:\n",
    "        dist = np.linalg.norm(centroids[key] - vector)\n",
    "        if dist < minDist:\n",
    "            minDist = dist\n",
    "            closest = key\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc117ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignToCluster(clusters, vectors, centroids):\n",
    "    for i in range(len(vectors)):\n",
    "        c = closestCluster(vectors[i], centroids)\n",
    "        clusters[c].append(i)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a038a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(k, max_iter, vectors):\n",
    "    clusters = {}\n",
    "    centroids = {}\n",
    "    idx = np.random.choice(len(vectors), k, replace=False)\n",
    "    for i in range(k):\n",
    "        clusters[i] = []\n",
    "        centroids[i] = vectors[idx[i]] \n",
    "    clusters = assignToCluster(clusters, vectors, centroids)\n",
    "    for _ in range(max_iter-1):\n",
    "        for i in range(k):\n",
    "            centroids[i] = np.zeros(len(alphabet))\n",
    "            for j in clusters[i]:\n",
    "                centroids[i] = centroids[i] + vectors[j]\n",
    "            if clusters[i] != []:\n",
    "                centroids[i] = centroids[i] / len(clusters[i])\n",
    "            if len(clusters[i]):\n",
    "                clusters[i].clear()\n",
    "        clusters = assignToCluster(clusters, vectors, centroids)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b0974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [i[0] for i in freq_sorted]\n",
    "clusters = dict()\n",
    "vectors = dict()\n",
    "for i in range(len(processed_tweets)):\n",
    "    vectors[i] = vectorize(processed_tweets[i], alphabet)\n",
    "clusters[0] = [i for i in range(len(processed_tweets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38018877",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans(4, 100, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ed52ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [23],\n",
       " 1: [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38],\n",
       " 2: [5],\n",
       " 3: [9]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4d202d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4765e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['touch',\n",
       " 'moment',\n",
       " '...',\n",
       " 'daughter',\n",
       " 'hug',\n",
       " 'father',\n",
       " 'return',\n",
       " 'delhi',\n",
       " 'win',\n",
       " 'long',\n",
       " 'battl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cccc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in clusters:\n",
    "    clusters[key] = np.array(clusters[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c3522e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([23]),\n",
       " 1: array([ 0,  1,  2,  3,  4,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "        37, 38]),\n",
       " 2: array([5]),\n",
       " 3: array([9])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58ce11fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c3cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
