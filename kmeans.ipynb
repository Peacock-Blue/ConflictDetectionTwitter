{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handy-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "import numpy as np\n",
    "from tweepy import OAuthHandler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8de553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ./auth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "departmental-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TwitterClient(object):\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.auth = OAuthHandler(os.getenv('api_key'), os.getenv('api_secret'))\n",
    "            self.auth.set_access_token(os.getenv('oauth_token'), os.getenv('oauth_token_secret'))\n",
    "            self.api = tweepy.API(self.auth)\n",
    "            assert self.api\n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "    \n",
    "    \n",
    "    def get_tweets(self, query, count = 10):\n",
    "        tweets = []\n",
    "        try:\n",
    "            fetched_tweets = self.api.search_tweets(q = query, count = count)\n",
    "            for tweet in fetched_tweets:\n",
    "                parsed_tweet = {}\n",
    "                parsed_tweet['text'] = tweet.text\n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
    "                if tweet.retweet_count > 0:\n",
    "                    if parsed_tweet not in tweets:\n",
    "                        tweets.append(parsed_tweet)\n",
    "                else:\n",
    "                    tweets.append(parsed_tweet)\n",
    "            return tweets\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"Error : \" + str(e))\n",
    "\n",
    "    def fetch_tweets(self, query, count = 10):\n",
    "        try:\n",
    "            return self.api.search_tweets(q = query, count = count)\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"Error : \" + str(e))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "attended-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = TwitterClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stock-acting",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TwitterClient' object has no attribute 'get_tweet_sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3badfc4179e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#Rhea'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_$_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-93b21cd92788>\u001b[0m in \u001b[0;36mget_tweets\u001b[0;34m(self, query, count)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mparsed_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mparsed_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mparsed_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tweet_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretweet_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparsed_tweet\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TwitterClient' object has no attribute 'get_tweet_sentiment'"
     ]
    }
   ],
   "source": [
    "with open('data.txt', 'w') as f:\n",
    "    tweets = api.get_tweets('#Rhea', 20000)\n",
    "    for tweet in tweets:\n",
    "        f.write(tweet + \"_$_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e51c00",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f59b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt', 'r')\n",
    "tweets = f.read().split('_$_')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7da8d",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fbcb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import download\n",
    "from collections import Counter\n",
    "\n",
    "# download('stopwords')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    tweets_eng = []\n",
    "    for word in tweets_clean:\n",
    "        flag = True\n",
    "        for i in word:\n",
    "            if ord(i) >= 256:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            tweets_eng.append(word)\n",
    "    return tweets_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05fd7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = [process_tweet(tweet) for tweet in tweets]\n",
    "while [] in processed_tweets:\n",
    "    processed_tweets.remove([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "637797be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bceb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tweet:list, freqs:dict, wordToTweet:dict):\n",
    "    for word in tweet:\n",
    "        if word in freqs:\n",
    "            freqs[word] += 1\n",
    "            wordToTweet[word].append(tweet)\n",
    "        else:\n",
    "            freqs[word] = 1\n",
    "            wordToTweet[word] = [tweet]\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65f75071",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = dict()\n",
    "wordToTweet = dict()\n",
    "for tweet in processed_tweets:\n",
    "    count_words(tweet, freqs, wordToTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e23828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a29dea07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rhea', 50),\n",
       " ('fe3h', 16),\n",
       " ('...', 9),\n",
       " ('fireemblem', 9),\n",
       " ('ke', 8),\n",
       " ('fireemblemthreehous', 7),\n",
       " ('seteth', 5),\n",
       " ('sushantsinghrajput', 4),\n",
       " ('one', 4),\n",
       " ('titan', 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_sorted = list(freqs.items())\n",
    "freq_sorted.sort(key = lambda x : -x[1])\n",
    "freq_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "048b699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweet, alphabet):\n",
    "    v = np.zeros(len(alphabet))\n",
    "    for i in range(len(alphabet)):\n",
    "        if alphabet[i] in tweet:\n",
    "            v[i] += 1\n",
    "    return v\n",
    "def closestCluster(vector, centroids):\n",
    "    closest = -1\n",
    "    minDist = 2**30\n",
    "    for key in centroids:\n",
    "        dist = np.linalg.norm(centroids[key] - vector)\n",
    "        if dist < minDist:\n",
    "            minDist = dist\n",
    "            closest = key\n",
    "    return closest\n",
    "def assignToCluster(clusters, vectors, centroids):\n",
    "    for i in range(len(vectors)):\n",
    "        c = closestCluster(vectors[i], centroids)\n",
    "        clusters[c].append(i)\n",
    "    return clusters\n",
    "def kmeans(k, max_iter, vectors):\n",
    "    clusters = {}\n",
    "    centroids = {}\n",
    "    idx = np.random.choice(len(vectors), k, replace=False)\n",
    "    for i in range(k):\n",
    "        clusters[i] = []\n",
    "        centroids[i] = vectors[idx[i]] \n",
    "    clusters = assignToCluster(clusters, vectors, centroids)\n",
    "    for _ in range(max_iter-1):\n",
    "        for i in range(k):\n",
    "            for j in clusters[i]:\n",
    "                centroids[i] = centroids[i] + vectors[j]\n",
    "            if clusters[i] != []:\n",
    "                centroids[i] = centroids[i] / len(clusters[i])\n",
    "            if len(clusters[i]):\n",
    "                clusters[i].clear()\n",
    "        clusters = assignToCluster(clusters, vectors, centroids)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def len_counts(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters.values()]\n",
    "    return dict(Counter(lens))\n",
    "\n",
    "\n",
    "def display_unique_tweets(tweets, cluster):\n",
    "    c_tweets = [tweets[i] for i in cluster]\n",
    "    for i in range(len(c_tweets)):\n",
    "        if not c_tweets[i] in c_tweets[:i]:\n",
    "            print(c_tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07b0974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [i[0] for i in freq_sorted]\n",
    "clusters = dict()\n",
    "vectors = dict()\n",
    "for i in range(len(processed_tweets)):\n",
    "    vectors[i] = vectorize(processed_tweets[i], alphabet)\n",
    "clusters[0] = [i for i in range(len(processed_tweets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38018877",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans(20, 100, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ed52ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: [],\n",
       " 2: [],\n",
       " 3: [24, 41, 42],\n",
       " 4: [0,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  26,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  32,\n",
       "  36,\n",
       "  39,\n",
       "  40,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65],\n",
       " 5: [],\n",
       " 6: [],\n",
       " 7: [],\n",
       " 8: [],\n",
       " 9: [],\n",
       " 10: [],\n",
       " 11: [],\n",
       " 12: [],\n",
       " 13: [],\n",
       " 14: [],\n",
       " 15: [],\n",
       " 16: [],\n",
       " 17: [3, 5, 6, 20, 21, 22, 23, 25, 27, 31, 33, 34, 35, 37, 38, 60],\n",
       " 18: [],\n",
       " 19: []}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4d202d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4765e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths:\n",
      "{0: 0, 1: 0, 2: 0, 3: 3, 4: 47, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 16, 18: 0, 19: 0}\n",
      "len counts:\n",
      "{0: 17, 3: 1, 47: 1, 16: 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"lengths:\")\n",
    "print({cluster:len(clusters[cluster]) for cluster in clusters})\n",
    "print(\"len counts:\")\n",
    "print(len_counts(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beagl', 'run', 'part', '4', 'seteth', 'neat', 'byleth', 'setleth', 'rhea', 'rhealeth', 'fe3h', 'fireemblemthreehous']\n",
      "['seteth', 'rhea', 'fe3h']\n",
      "['children', 'goddess', 'go', 'trick', 'treat', 'fe3h', 'fehero', 'fireemblem', 'rhea', 'seteth']\n",
      "['btw', 'final', 'cave', 'wrote', 'pine', 'rheagard', '<3', '3', 'rheagard', 'edelgard', 'rhea', 'fe3h']\n",
      "['seiro', 'fire', 'emblem', 'feel', 'fe3h', 'rhea']\n",
      "['->', 'fe3h', 'fire_emblem', 'rhea']\n",
      "['get', 'loser', 'go', 'church', 'fe3h', 'seteth', 'flayn', 'rhea']\n",
      "['immacul', 'one', 'fe3h', 'rhea', 'archbishoprhea']\n",
      "['ladi', 'rhea', 'fanart', 'wip', 'rhea', 'fireemblem', 'fe3h', 'fehero', 'feh', 'jrpg']\n",
      "['new', 'year', 'new', 'draw', 'manga', 'mangaka', 'anim', 'animeart', 'mangaart', 'fe3h', 'fireemblemthreehous', 'fireemblem', 'nintendo']\n",
      "[\"he'\", 'gone', 'mother', '...', 'rhea', 'fe3h']\n",
      "['rhea', 'definit', 'problem', 'fireemblem', 'threehous', 'fe3h', 'fireemblemthreehous', 'rhea']\n",
      "['halloween', 'rhea', 'beauti', '...', 'feh', 'rhea', 'fireemblemhero', 'fireemblem', 'fe3h']\n",
      "['rel', 'quick', 'rhea', 'draw', 'rhea', 'fireemblem', 'fireemblemthreehous', 'fe', 'fe3h', 'illustr', 'draw', 'art', 'fanart', 'gami']\n",
      "['twitter', 'happen', 'know', '.\\n.\\n.', 'art', 'fe3h', 'rheafireemblem', 'rhea', 'edelgard', 'fireemblemthreehous', 'edelg']\n"
     ]
    }
   ],
   "source": [
    "display_unique_tweets(processed_tweets, clusters[17])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
