{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ./auth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TwitterClient(object):\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.auth = OAuthHandler(os.getenv('api_key'), os.getenv('api_secret'))\n",
    "            self.auth.set_access_token(os.getenv('oauth_token'), os.getenv('oauth_token_secret'))\n",
    "            self.api = tweepy.API(self.auth)\n",
    "            assert self.api\n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "    \n",
    "    \n",
    "    def get_tweets(self, query, count = 10):\n",
    "        tweets = []\n",
    "        try:\n",
    "            fetched_tweets = self.api.search_tweets(q = query, count = count)\n",
    "            for tweet in fetched_tweets:\n",
    "                parsed_tweet = {}\n",
    "                parsed_tweet['text'] = tweet.text\n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
    "                if tweet.retweet_count > 0:\n",
    "                    if parsed_tweet not in tweets:\n",
    "                        tweets.append(parsed_tweet)\n",
    "                else:\n",
    "                    tweets.append(parsed_tweet)\n",
    "            return tweets\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"Error : \" + str(e))\n",
    "\n",
    "    def fetch_tweets(self, query, count = 10):\n",
    "        try:\n",
    "            return self.api.search_tweets(q = query, count = count)\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"Error : \" + str(e))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TwitterClient()\n",
    "#tc.fetch_tweets('#FarmLaws',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The tweets have been fetched and stored in a file. \n",
    "## Use the cached tweets for consistent results instead of fetching new from twitter.\n",
    "with open('divtext.txt', 'w') as f:\n",
    "    tweets = tc.fetch_tweets('#FarmLaws',8000)\n",
    "    for tweet in tweets:\n",
    "        f.write(tweet.text + \"_$_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Fetch from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('divtext.txt', 'r')\n",
    "tweets = f.read().split('_$_')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @suchitraarya: देखो जीतकर हळधर जा रहा है,\\nमात सत्ता का अभिमान खा रहा है।\\n#FarmersProtest_FatehMarch \\n#FarmersProtest \\n#FarmLaws https://…'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Preprocess each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import download\n",
    "# download('stopwords')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    tweets_eng = []\n",
    "    for word in tweets_clean:\n",
    "        flag = True\n",
    "        for i in word:\n",
    "            if ord(i) >= 256:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            tweets_eng.append(word)\n",
    "    return tweets_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @suchitraarya: देखो जीतकर हळधर जा रहा है,\n",
      "मात सत्ता का अभिमान खा रहा है।\n",
      "#FarmersProtest_FatehMarch \n",
      "#FarmersProtest \n",
      "#FarmLaws https://…\n",
      "['farmersprotest_fatehmarch', 'farmersprotest', 'farmlaw']\n"
     ]
    }
   ],
   "source": [
    "processed_tweets = [process_tweet(tweet) for tweet in tweets]\n",
    "print(tweets[0])\n",
    "print(processed_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Word counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tweet:list, freqs:dict, wordToTweet:dict):\n",
    "    for word in tweet:\n",
    "        if word in freqs:\n",
    "            freqs[word] += 1\n",
    "            wordToTweet[word].append(tweet)\n",
    "        else:\n",
    "            freqs[word] = 1\n",
    "            wordToTweet[word] = [tweet]\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = dict()\n",
    "wordToTweet = dict()\n",
    "for tweet in processed_tweets:\n",
    "    count_words(tweet, freqs, wordToTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(freqs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install numpy pandas seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs.keys()\n",
    "# list(freqs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freqs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('farmlaw', 33)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_word = np.argmax(list(freqs.values()))\n",
    "list(freqs.items())[most_freq_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('farmlaw', 33),\n",
       " ('farmer', 23),\n",
       " ('stopbjp_goonda', 16),\n",
       " ('dmk', 16),\n",
       " ('protest', 15),\n",
       " ('border', 12),\n",
       " ('year-long', 9),\n",
       " ('farmlawsrep', 8),\n",
       " ('farmersprotest', 7),\n",
       " ('..', 7)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sorted in descending order of frequencies\n",
    "freq_sorted = list(freqs.items())\n",
    "freq_sorted.sort(key = lambda x : -x[1])\n",
    "freq_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweet, alphabet):\n",
    "    v = np.zeros(len(alphabet))\n",
    "    for i in range(len(alphabet)):\n",
    "        if alphabet[i] in tweet:\n",
    "            v[i] += 1\n",
    "    return v\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.power(np.sum(np.power(v,2)), 0.5)\n",
    "    if norm == 0:\n",
    "        return 0\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def len_counts(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters.values()]\n",
    "    return dict(Counter(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_dist(clusterA, clusterB, vectors):\n",
    "    dist_sum = 0\n",
    "    for i in clusterA:\n",
    "        for j in clusterB:\n",
    "            dist_sum += np.sum(np.power(vectors[i] - vectors[j], 2))\n",
    "    \n",
    "    dist_avg = dist_sum / (len(clusterA) * len(clusterB))\n",
    "    return dist_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeOnce(clusters, vectors):\n",
    "    min_dist = 2**30\n",
    "    min_loc = (0, 0)\n",
    "    for clusterInxA in clusters:\n",
    "        for clusterInxB in clusters:\n",
    "            if clusterInxA != clusterInxB:\n",
    "                dist = get_avg_dist(clusters[clusterInxA], clusters[clusterInxB], vectors)\n",
    "                if dist < min_dist:\n",
    "                    min_loc = (clusterInxA, clusterInxB)\n",
    "                    min_dist = dist\n",
    "\n",
    "    clusters[min_loc[0]] = clusters[min_loc[0]] + clusters[min_loc[1]]\n",
    "    clusters.pop(min_loc[1])\n",
    "\n",
    "def mergeToK(clusters, vectors, K):\n",
    "    while len(clusters.keys()) > K:\n",
    "        mergeOnce(clusters, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique_tweets(tweets, cluster):\n",
    "    c_tweets = [tweets[i] for i in cluster]\n",
    "    for i in range(len(c_tweets)):\n",
    "        if not c_tweets[i] in c_tweets[:i]:\n",
    "            print(c_tweets[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [i[0] for i in freq_sorted]\n",
    "n = len(processed_tweets)\n",
    "m = len(alphabet)\n",
    "clusters = dict()\n",
    "vectors = dict()\n",
    "for i in range(n):\n",
    "    vectors[i] = vectorize(processed_tweets[i], alphabet)\n",
    "    clusters[i] = [i]\n",
    "mergeToK(clusters, vectors, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 51, 81, 93, 55, 11, 25, 3, 13, 35, 39, 52, 59, 63, 64, 69, 71, 72, 73, 74, 75, 76, 77, 9, 21, 54, 12, 18, 28, 33, 40, 47, 50, 53, 57, 66, 67, 68, 84, 85, 91, 94, 95, 97, 99, 100, 38, 80, 20, 45, 78, 88, 79, 65, 48, 62, 49, 6, 42, 36, 26, 1, 5, 10, 8, 14, 82, 37, 24, 83, 15, 17, 23, 29, 30, 89, 27, 44, 43, 58, 46, 34, 61, 92], 2: [2], 4: [4], 7: [7, 31, 41, 60], 16: [16], 19: [19], 22: [22], 32: [32], 56: [56], 70: [70], 86: [86], 87: [87], 90: [90], 96: [96], 98: [98]}\n"
     ]
    }
   ],
   "source": [
    "# print(\"lengths:\")\n",
    "# print({cluster:len(clusters[cluster]) for cluster in clusters})\n",
    "# print(\"len counts:\")\n",
    "# print(len_counts(clusters))\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pm', 'modi', 'lost', 'farmlaw', 'battl', 'jan', '28th', 'night', 'polic', 'reach', 'rakesh', 'tikait', 'ghazipur', 'border']\n"
     ]
    }
   ],
   "source": [
    "display_unique_tweets(processed_tweets, clusters[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divisive Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweet, alphabet):\n",
    "    v = np.zeros(len(alphabet))\n",
    "    for i in range(len(alphabet)):\n",
    "        if alphabet[i] in tweet:\n",
    "            v[i] += 1\n",
    "    return v\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.power(np.sum(np.power(v,2)), 0.5)\n",
    "    if norm == 0:\n",
    "        return 0\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMostDistant(cluster, vectors):\n",
    "    if len(cluster)<=1:\n",
    "        return -1\n",
    "    maxDist = -1\n",
    "    item = 0\n",
    "    n = len(cluster)\n",
    "    for i in cluster:\n",
    "        dist = 0\n",
    "        for j in cluster:\n",
    "            dist += np.linalg.norm(vectors[i] - vectors[j])\n",
    "        dist /= n\n",
    "        if dist>maxDist:\n",
    "            maxDist = dist\n",
    "            item = i\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCloserToFirstCluster(cluster1, cluster2, item, vectors):\n",
    "    dist1 = 0\n",
    "    dist2 = 0\n",
    "    n1 = len(cluster1)\n",
    "    n2 = len(cluster2)\n",
    "    for i in cluster1:\n",
    "        dist1 += np.linalg.norm(vectors[i] - vectors[item])\n",
    "    dist1 /= n1\n",
    "    for i in cluster2:\n",
    "        dist2 += np.linalg.norm(vectors[i] - vectors[item])\n",
    "    dist2 /= n2\n",
    "    if dist1<=dist2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dClusteringOnce(clusters, vectors):\n",
    "    tmp = []\n",
    "    for key in clusters:\n",
    "        tmp.append(key)\n",
    "    for key in tmp:\n",
    "        item = findMostDistant(clusters[key], vectors)\n",
    "        if item == -1:\n",
    "            continue\n",
    "        if item==key:\n",
    "            item = clusters[key][1]\n",
    "            clusters[item] = clusters[key][1:]\n",
    "            for i in clusters[key][1:]:\n",
    "                if isCloserToFirstCluster(clusters[item], clusters[key], i, vectors):\n",
    "                    clusters[item].append(i)\n",
    "                    clusters[key].remove(i)\n",
    "            continue\n",
    "        clusters[item] = [item]\n",
    "        clusters[key].remove(item)\n",
    "        for i in clusters[key]:\n",
    "            if isCloserToFirstCluster(clusters[item], clusters[key], i, vectors):\n",
    "                clusters[item].append(i)\n",
    "                clusters[key].remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dClusteringMinK(clusters, vectors, k):\n",
    "    while len(clusters)<k:\n",
    "        dClusteringOnce(clusters, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['farmersprotest_fatehmarch', 'farmersprotest', 'farmlaw'], ['farmlaw', 'farmersprotest_fatehmarch', 'farmlawsrep', 'farmerswon', 'kisanandolan', 'kisan'], ['pm', 'modi', 'lost', 'farmlaw', 'battl', 'jan', '28th', 'night', 'polic', 'reach', 'rakesh', 'tikait', 'ghazipur', 'border'], ['stopbjp_goonda', 'dmk'], ['modi', 'govt', 'repeal', 'farmlaw', 'nation', 'interest', '..', 'everi', 'one', 'know', 'fact', 'modi', 'ji', 'never', 'back', 'h'], ['farmlaw', 'farmersprotest_fatehmarch', 'farmlawsrep', 'farmerswon', 'kisanandolan', 'kisan'], ['nh', '24', 'farmlaw', 'farmerprotest'], ['farmer', 'protest', 'end', '..', 'panoram', 'view', 'singhu', 'border', 'area', 'show', 'farmer', 'leav', 'year-long', 'agitati'], ['farmlaw', 'farmersprotest_fatehmarch', 'farmlawsrep', 'farmerswon', 'kisanandolan', 'kisanmahapancha'], ['farmlaw', 'farmersp'], ['farmlaw', 'farmersprotest_fatehmarch', 'farmlawsrep', 'farmerswon', 'kisanandolan', 'kisan'], ['farmlaw', 'protest', 'farmer'], ['stopbjp_goonda', 'dmk'], ['farmlaw', 'farmersprotest_fatehmarch', 'farmlawsrep', 'farmerswon'], ['protest', 'behind', 'triumphant', 'farmer', 'start', 'return', 'home', 'samyuktkisanmorcha', 'tikribord'], ['would', 'believ', 'say', 'economist', 'predict', 'withdraw', 'farmlaw', 'long', 'back', 'dec', '2020', 'well', 'happ'], ['protest', 'shown', 'us', 'mean', 'commun', 'receiv', 'sign', 'letter', 'govern'], ['thread', 'tribun', 'front', 'page', 'stori', 'farmlaw', 'victori', 'farmer', 'head', 'home', 'year-long', 'stir', 'given', 'warm'], ['singhubord', 'far'], ['farmlaw'], ['tremend', 'victori', '..', 'ye', 'definit', 'victori', 'cherish', 'whole', 'countri', '..', 'india', 'farmlawsrep', 'f'], ['protest', 'shown', 'us', 'mean', 'commun', 'receiv', 'sign', 'letter', 'govern'], ['roll', '...\\n.\\n.\\n.\\n.', 'anwertoon', 'currentaffair', 'politicalsatir', 'humour', 'thehindu', 'thetimesofindia'], ['farmlaw', 'protest', 'farmer'], ['farmlaw', 'agri', 'start', 'up', 'tech'], ['fair', 'point', 'court', 'known', 'succumb', 'street', 'violenc', 'saw', 'farmlaw', 'wer'], ['protest', 'shown', 'us', 'mean', 'commun', 'receiv', 'sign', 'letter', 'gov'], ['congratul', 'pm', 'ji', 'honor', 'wish', 'farmer', 'democraci', 'peopl', 'wish', 'prevail', 'farmlaw'], ['farmer', 'protest', 'end', '..', 'panoram', 'view', 'singhu', 'border', 'area', 'show', 'farmer', 'leav', 'year-long', 'agitati'], ['restaur', 'owner', 'provid', 'langar', 'farmer', 'singhu', 'border', 'year', 'set', 'reopen', 'eateri', 'farmersprotest'], ['farmer', 'leav', 'site', 'protest', 'ghazipur', 'border', 'delhi-up', 'border', 'suspend', 'year-long', 'protest', '3'], ['stopbjp_goonda', 'dmk'], ['delhi', 'border', 'haryana', 'kisanmajdoorektazindabaa'], ['modihaitomumkinhai', 'modi_job_do', 'modiaccounthack', 'farmlaw', 'farmlawsrep', 'kanganaranuat', 'katrinakaifwed'], ['pm'], ['stopbjp_goonda', 'dmk'], ['farmer', 'protest', 'end', '..', 'panoram', 'view', 'singhu', 'border', 'area', 'show', 'farmer', 'leav', 'year-long', 'agitati'], ['nh', '24', 'farmlaw', 'farmerprotest'], ['repeal', 'farm', 'law', 'movement', 'achiev', 'lot', 'social', 'polit', 'gain', 'far'], ['rahul', 'practis', 'satyagraha', 'motiv', 'farmer', 'hold', 'firmli', 'truth', 'peac', 'democ'], ['singhubord', 'far'], ['weekendkavaar', 'biggboss', '15', 'rashmidesai', 'trend', 'yehhaichahatein', 'umarriaz', 'ipl', '2022retent', 'omicron'], ['badikhabar', '13', 'farmlaw'], ['badikhabar', '13', 'farmlaw', 'farmerp'], ['farmersprotest', 'farmlaw'], ['stopbjp_goonda', 'dmk'], ['covidvari', 'farmlaw'], ['farmer', 'farmersprotest', 'farmlaw', 'farmlawsrepea'], ['last', 'muslim', 'end', 'time', 'qiyamah', 'sign', '...', 'prophet', 'muhammad', 'saw', 'deep', 'sad', 'condit', 'mu'], ['repeal', 'farm', 'law', 'movement', 'achiev', 'lot', 'social', 'polit', 'gain', 'far'], ['stopbjp_goonda', 'dmk'], ['farmer', 'protest', 'end', '..', 'panoram', 'view', 'singhu', 'border', 'area', 'show', 'farmer', 'leav', 'year-long', 'agi'], ['farmer', 'start', 'remov', 'settlement', \"delhi'\", 'tikri', 'border', 'suspens', 'year-long', 'protest', '3'], ['badikhabar', '13', 'farmlaw'], ['stopbjp_goonda', 'dmk'], ['stopbjp_goonda', 'dmk'], ['farmlawsrep', 'farmlaw', 'farm'], ['stopbjp_goonda', 'dmk'], ['farmer', 'dismantl', 'makeshift', 'camp', 'delhi-haryana', 'state', 'border', 'singhu', 'dec', '11', '2021', 'head', 'hom'], ['stopbjp_goonda', 'dmk'], ['stopbjp_goonda', 'dmk'], ['stopbjp_goonda', 'dmk'], ['stopbjp_goonda', 'dmk'], ['stopbjp_goonda', 'dmk'], ['stopbjp_goonda', 'dmk'], ['stopbjp_goonda', 'dmk'], ['live', 'baattochubhegi'], ['1925', '2021'], ['pm'], ['farmersprotest', 'farmlaw'], ['serious', 'rofl', 'fm', 'convict', 'farmlaw', 'privatis'], ['dear', 'ji', 'farmlaw', 'repeal', 'controversi', 'nmc', 'guidelin', 'affect', 'liv'], ['3', 'week', 'prime', 'minist', 'modi', 'announc', 'repeal', 'farmlaw', 'protest', 'farmer', 'final', 'end', 'year-long', 'ag'], ['ironi', 'system', 'centr', 'know', 'much', 'farmersprotest', 'cost', 'nhai', 'idea', 'mani', 'live'], ['live', 'baattochubhegi'], ['congratul', 'pm', 'ji', 'honor', 'wish', 'farmer', 'democraci', 'peopl', 'wish', 'prevail', 'farmlaw'], ['thank', 'peopl', 'continu', 'support', 'farmer', 'day', '1st', 'huge', 'obstacl', 'till', 'win', 'like'], ['farmer', 'start', 'remov', 'settlement', \"delhi'\", 'tikri', 'border', 'suspens', 'year-long', 'protest', '3'], ['farmersprotest', 'farmlaw'], ['yeah', 'make', 'like', 'virtu', 'give', 'inch', 'peopl', 'comeback', 'mile', 'india', 'regret', \"modi'\"], ['effort', 'banker', 'gain', 'mass', 'godi', 'media', 'other', 'may', 'ntt', 'show', 'this.but', 'slowli', 'becom', 'nation', 'iss']]\n"
     ]
    }
   ],
   "source": [
    "while [] in processed_tweets:\n",
    "    processed_tweets.remove([])\n",
    "print(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [i[0] for i in freq_sorted]\n",
    "clusters = dict()\n",
    "vectors = dict()\n",
    "for i in range(len(processed_tweets)):\n",
    "    vectors[i] = vectorize(processed_tweets[i], alphabet)\n",
    "clusters[0] = [i for i in range(len(processed_tweets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78], 80: [80], 4: [4], 48: [48], 2: [2], 17: [17], 58: [58], 15: [15], 72: [72], 79: [79]}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dClusteringMinK(clusters, vectors, 7)\n",
    "print(clusters)\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
