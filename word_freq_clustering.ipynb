{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /home/amor/.local/lib/python3.9/site-packages (4.4.0)\n",
      "Requirement already satisfied: textblob in /home/amor/.local/lib/python3.9/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/amor/.local/lib/python3.9/site-packages (from textblob) (3.6.3)\n",
      "Requirement already satisfied: click in /home/amor/.local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /home/amor/.local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: regex in /home/amor/.local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2021.9.24)\n",
      "Requirement already satisfied: joblib in /home/amor/.local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in /home/amor/.local/lib/python3.9/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.11.1 in /usr/lib/python3/dist-packages (from tweepy) (2.25.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib<2,>=1.0.0->tweepy) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tweepy textblob\n",
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ./auth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TwitterClient(object):\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.auth = OAuthHandler(os.getenv('api_key'), os.getenv('api_secret'))\n",
    "            self.auth.set_access_token(os.getenv('oauth_token'), os.getenv('oauth_token_secret'))\n",
    "            self.api = tweepy.API(self.auth)\n",
    "            assert self.api\n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    "    \n",
    "    \n",
    "    def get_tweets(self, query, count = 10):\n",
    "        tweets = []\n",
    "        try:\n",
    "            fetched_tweets = self.api.search_tweets(q = query, count = count)\n",
    "            for tweet in fetched_tweets:\n",
    "                parsed_tweet = {}\n",
    "                parsed_tweet['text'] = tweet.text\n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
    "                if tweet.retweet_count > 0:\n",
    "                    if parsed_tweet not in tweets:\n",
    "                        tweets.append(parsed_tweet)\n",
    "                else:\n",
    "                    tweets.append(parsed_tweet)\n",
    "            return tweets\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"Error : \" + str(e))\n",
    "\n",
    "    def fetch_tweets(self, query, count = 10):\n",
    "        try:\n",
    "            return self.api.search_tweets(q = query, count = count)\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"Error : \" + str(e))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TwitterClient()\n",
    "#tc.fetch_tweets('#FarmLaws',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The tweets have been fetched and stored in a file. \n",
    "## Use the cached tweets for consistent results instead of fetching new from twitter.\n",
    "with open('farmer.txt', 'w') as f:\n",
    "    tweets = tc.fetch_tweets('#FarmLaws',8000)\n",
    "    for tweet in tweets:\n",
    "        f.write(tweet.text + \"_$_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Fetch from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('farmer.txt', 'r')\n",
    "tweets = f.read().split('_$_')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @Jasvenkaur: #FarmLaws Solution @mssirsa Is \\n#KhalistanReferendum NOT @BJP4India @TajinderBagga \\n#SFJ Dares @JPNadda @AmitShah @mssirsa…'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Preprocess each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import download\n",
    "# download('stopwords')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    tweets_eng = []\n",
    "    for word in tweets_clean:\n",
    "        flag = True\n",
    "        for i in word:\n",
    "            if ord(i) >= 256:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            tweets_eng.append(word)\n",
    "    return tweets_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Jasvenkaur: #FarmLaws Solution @mssirsa Is \n",
      "#KhalistanReferendum NOT @BJP4India @TajinderBagga \n",
      "#SFJ Dares @JPNadda @AmitShah @mssirsa…\n",
      "['farmlaw', 'solut', 'khalistanreferendum', 'sfj', 'dare']\n"
     ]
    }
   ],
   "source": [
    "processed_tweets = [process_tweet(tweet) for tweet in tweets]\n",
    "print(tweets[0])\n",
    "print(processed_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Word counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tweet:list, freqs:dict, wordToTweet:dict):\n",
    "    for word in tweet:\n",
    "        if word in freqs:\n",
    "            freqs[word] += 1\n",
    "            wordToTweet[word].append(tweet)\n",
    "        else:\n",
    "            freqs[word] = 1\n",
    "            wordToTweet[word] = [tweet]\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = dict()\n",
    "wordToTweet = dict()\n",
    "for tweet in processed_tweets:\n",
    "    count_words(tweet, freqs, wordToTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(freqs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install numpy pandas seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs.keys()\n",
    "# list(freqs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freqs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('farmlaw', 80)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_word = np.argmax(list(freqs.values()))\n",
    "list(freqs.items())[most_freq_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('farmlaw', 80),\n",
       " ('sfj', 72),\n",
       " ('solut', 61),\n",
       " ('khalistanreferendum', 60),\n",
       " ('dare', 43),\n",
       " ('account', 23),\n",
       " ('bjp', 16),\n",
       " ('sirsa', 15),\n",
       " ('law', 12),\n",
       " ('intern', 11)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sorted in descending order of frequencies\n",
    "freq_sorted = list(freqs.items())\n",
    "freq_sorted.sort(key = lambda x : -x[1])\n",
    "freq_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweet, alphabet):\n",
    "    v = np.zeros(len(alphabet))\n",
    "    for i in range(len(alphabet)):\n",
    "        if alphabet[i] in tweet:\n",
    "            v[i] += 1\n",
    "    return v\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.power(np.sum(np.power(v,2)), 0.5)\n",
    "    if norm == 0:\n",
    "        return 0\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def len_counts(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters.values()]\n",
    "    return dict(Counter(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_dist(clusterA, clusterB, vectors):\n",
    "    dist_sum = 0\n",
    "    for i in clusterA:\n",
    "        for j in clusterB:\n",
    "            dist_sum += np.sum(np.power(vectors[i] - vectors[j], 2))\n",
    "    \n",
    "    dist_avg = dist_sum / (len(clusterA) * len(clusterB))\n",
    "    return dist_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeOnce(clusters, vectors):\n",
    "    min_dist = 2**30\n",
    "    min_loc = (0, 0)\n",
    "    for clusterInxA in clusters:\n",
    "        for clusterInxB in clusters:\n",
    "            if clusterInxA != clusterInxB:\n",
    "                dist = get_avg_dist(clusters[clusterInxA], clusters[clusterInxB], vectors)\n",
    "                if dist < min_dist:\n",
    "                    min_loc = (clusterInxA, clusterInxB)\n",
    "                    min_dist = dist\n",
    "\n",
    "    clusters[min_loc[0]] = clusters[min_loc[0]] + clusters[min_loc[1]]\n",
    "    clusters.pop(min_loc[1])\n",
    "\n",
    "def mergeToK(clusters, vectors, K):\n",
    "    while len(clusters.keys()) > K:\n",
    "        mergeOnce(clusters, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique_tweets(tweets, cluster):\n",
    "    c_tweets = [tweets[i] for i in cluster]\n",
    "    for i in range(len(c_tweets)):\n",
    "        if not c_tweets[i] in c_tweets[:i]:\n",
    "            print(c_tweets[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [i[0] for i in freq_sorted]\n",
    "n = len(processed_tweets)\n",
    "m = len(alphabet)\n",
    "clusters = dict()\n",
    "vectors = dict()\n",
    "for i in range(n):\n",
    "    vectors[i] = vectorize(processed_tweets[i], alphabet)\n",
    "    clusters[i] = [i]\n",
    "mergeToK(clusters, vectors, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths:\n",
      "{0: 53, 3: 4, 5: 3, 6: 1, 15: 8, 16: 6, 19: 1, 31: 1, 36: 3, 41: 1, 43: 8, 44: 1, 47: 2, 48: 1, 60: 1}\n",
      "len counts:\n",
      "{53: 1, 4: 1, 3: 2, 1: 7, 8: 2, 6: 1, 2: 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"lengths:\")\n",
    "print({cluster:len(clusters[cluster]) for cluster in clusters})\n",
    "print(\"len counts:\")\n",
    "print(len_counts(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sfj', 'make', 'account', 'sirsa', 'new', 'fellow', 'bjp', 'farmlaw', 'khalistanreferendum', 'sfj']\n"
     ]
    }
   ],
   "source": [
    "display_unique_tweets(processed_tweets, clusters[60])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
